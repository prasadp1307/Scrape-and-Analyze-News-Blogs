Scrape and Analyze News Blogs

Natural language processing (NLP) refers to the branch of computer science—and more specifically, the branch of artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much the same way human beings can.  
    
What is NLTK used for?

The Natural Language Toolkit (NLTK) is a platform used for building Python programs that work with human language data for applying in statistical natural language processing (NLP). It contains text processing libraries for tokenization, parsing, classification, stemming, tagging and semantic reasoning.  
    
What is web scraping?

Web scraping is the process of using bots to extract content and data from a website. Unlike screen scraping, which only copies pixels displayed onscreen, web scraping extracts underlying HTML code and, with it, data stored in a database. The scraper can then replicate entire website content elsewhere.

 Task : In this project we have to fetch news articles on basis of topics given.

Topics : Zomato,Amazon,Flipkart,Swiggy

Algorithms or libraries used : NLTK , Newspaper

Website : 1. www.ndtv.com  (five news blogs on zomato)
          2. www.opindia.com (five news blogs on Amazon)
          
 Challenges Faced:

        The most popular scraping internet data challenge. Websites can decide whether they will give bots access to clean data. Some sites forbid automatic data collection. The reasons for the ban can be completely different. If you come across a website that prohibits collection through its robots.txt, follow fair play principles and ask the site owner for permission to collect data. Otherwise, it is better to look for an alternative site with similar information. 

        Websites may be slow to load content or may not load at all when receiving a large number of access requests. In such a situation, you can refresh the page and wait for the site to recover. However, the parser will not know how to handle such a situation and data collection may be interrupted. 

        Another website challenge you have to face when scraping. Designers may have their design standards when creating web pages, so page structures will vary. Websites also undergo periodic changes to improve user interaction or add new features. This often results in structural changes to the web page itself. Web parsers are created with page code elements in mind, so these changes make the codes more complex, which affects how the parsers work.

And because they are customized to a specific page design, they won’t work for the updated page. Sometimes even a minor change requires a new parser configuration.

System Breakdown.         
